

from glob import glob
from subprocess import call, check_output
import os


HOME_DIR = os.path.expanduser("~")
MAIN_DIR = os.path.join(HOME_DIR, '{{cookiecutter.projects_base}}/{{cookiecutter.project_name}}')
DATA_DIR = os.path.join(MAIN_DIR, 'data')
RNASEQ_DIR=os.path.join(DATA_DIR, 'rnaseq')
PICARD_DIR = os.path.join(DATA_DIR, 'picard')

NUM_THREADS_PER_SAMPLE={{cookiecutter.number_threads_per_sample}}

STAR_EXECUTABLE="STAR{{cookiecutter.star_version}}"
BOWTIE2_EXECUTABLE="bowtie2-{{cookiecutter.bowtie2_version}}"
FASTQC_EXECUTABLE="fastqc{{cookiecutter.fastqc_version}}"
FEATURECOUNTS_EXECUTABLE="featureCounts{{cookiecutter.featurecounts_version}}"
PICARD_EXECUTABLE="/opt/picard-tools-{{cookiecutter.picard_version}}/picard.jar"

SPECIES=[]
ENSEMBL_DIR=[]
MAPPER_INDEX=[]
BOWTIE_INDEX=[]
GTF_FILE=[]
REF_FLAT=[]
STRAND=None

DATA_TYPE = "{{cookiecutter.data_type}}"
if DATA_TYPE == "rnaseq":
     MAPPER_EXECUTABLE=STAR_EXECUTABLE
else:
    MAPPER_EXECUTABLE=BOWTIE2_EXECUTABLE

{% for s in cookiecutter.species.split(' ') %}
SPECIES.append("{{ s }}")
ENSEMBL_DIR.append("%s/{{ s }}_ensembl_{{cookiecutter.ensembl_version}}" % DATA_DIR)
if DATA_TYPE == "rnaseq":
    MAPPER_INDEX.append("%s/{{ s }}_ensembl_{{cookiecutter.ensembl_version}}/STAR_indices/{{cookiecutter.assembly_names[s]}}_{{cookiecutter.star_version}}" % DATA_DIR)
else:
    MAPPER_INDEX.append("%s/{{ s }}_ensembl_{{cookiecutter.ensembl_version}}/BOWTIE2_indices/{{cookiecutter.assembly_names[s]}}_{{cookiecutter.bowtie2_version}}" % DATA_DIR)
GTF_FILE.append("%s/{{ s }}_ensembl_{{cookiecutter.ensembl_version}}/{{cookiecutter.gtf_files[s]}}" % DATA_DIR)
REF_FLAT.append("%s/{{ s }}/{{cookiecutter.rff_files[s]}}" % PICARD_DIR)
{% endfor %}


STRATEGY="{{cookiecutter.strategy}}"


NUM_THREADS_PER_SAMPLE=8
IS_PAIRED_END="{{cookiecutter.paired_end_read}}"
gtf_dict = dict(zip(SPECIES, GTF_FILE))
SAMPLES="{{cookiecutter.rnaseq_samples}}"
SAMPLES=SAMPLES.split()

READ1_SUFFIX = ''
READ2_SUFFIX = ''
read_identifiers =  ["{{ cookiecutter.read1_identifier }}", "{{ cookiecutter.read2_identifier }}"]
no_identity=[i == '*' for i in read_identifiers]
if all(no_identity):
    READ1_SUFFIX=''
    READ2_SUFFIX=''
else:
    READ1_SUFFIX = read_identifiers[0]
    READ2_SUFFIX = read_identifiers[1]


def pick_first_sample():
    test_sample = SAMPLES[0]
    test_species = SPECIES[0]
    test_gtf = GTF_FILE[0]
    test_bam = os.path.join("results/final_bams", "%s.%s.bam" % (test_sample, test_species))
    return test_bam, test_gtf


def run_featurecounts(strand_setting):
    read_counts_folder = "results/read_counts"
    if not os.path.exists(read_counts_folder):
        os.makedirs(read_counts_folder)
    test_bam, test_gtf = pick_first_sample()
    outfile = os.path.join(read_counts_folder, "counts_temp.%s.out" % strand_setting)
    call("featureCounts -T 4 -p -a %s -o %s -s %s %s" % (test_gtf, outfile, strand_setting, test_bam), shell=True)
    outfile_old = ".".join([outfile, "summary"])
    outfile_new = ".".join([outfile, "testsummary"])
    call("mv %s %s" % (outfile_old, outfile_new), shell=True)
    return outfile_new


def get_assigned_reads(counts_file):
    counts = check_output("grep Assigned %s | awk '{print $2}'" % counts_file, universal_newlines=True, shell=True)
    counts = [int(i) for i in counts.split('\n') if i][0]
    return counts


def calculate_strandedness(strand_dict):
    strand_zero = strand_dict["0"]
    strand_one = strand_dict["1"]
    strand_two = strand_dict["2"]
    if all(v == 0 for v in list(strand_dict.values())):
        raise Exception('No mapped reads detected')
    difference = (float(strand_one) - float(strand_two))/(float(strand_one) + float(strand_two))
    if abs(difference) < 0.75:
        return "0"
    elif difference >= 0.75:
        return "1"
    elif difference <= -0.75:
        return "2"
    else:
        raise Exception('Can not calculate strandedness from values: one=%s, two=%s, three=%s' % (strand_zero, strand_one, strand_two))


def strand_test(picard=False):
    picard_strand_value = {"0": "NONE", "1": "FIRST_READ_TRANSCRIPTION_STRAND", "2": "SECOND_READ_TRANSCRIPTION_STRAND"}
    if STRAND is None:
        strand_assigned_reads = {"0": 0, "1": 0, "2": 0}
        for i in strand_assigned_reads:
            counts_file = run_featurecounts(i)
            assigned = get_assigned_reads(counts_file)
            strand_assigned_reads[i] = assigned
        strandedness = calculate_strandedness(strand_assigned_reads)
    else:
        strandedness = STRAND
    if picard is True:
        return picard_strand_value[strandedness]
    return strandedness


def retrieve_fastqs(sample):
    fastq = glob("data/rnaseq/%s/*{{ cookiecutter.fastq_suffix }}" % sample)
    return fastq


def species_index():
    assert len(SPECIES) == len(MAPPER_INDEX)
    species_index_pairs = []
    for i in range(len(SPECIES)):
        species_index_pair = " ".join([SPECIES[i], MAPPER_INDEX[i]])
        species_index_pairs.append(species_index_pair)
    species_index_pairs = " ".join(species_index_pairs)
    return species_index_pairs




rule fastqc:
    input:
        fastq = lambda wildcards: retrieve_fastqs(wildcards.sample)
    output:
        "results/fastqc/{sample}/stdin_fastqc.html"
    params:
        output_dir = lambda wildcards, output: os.path.dirname(output[0]),
        fqc = FASTQC_EXECUTABLE
    log: "results/logs/fastqc/{sample}.log"
    threads: NUM_THREADS_PER_SAMPLE
    shell:
        """
        zcat {input.fastq} | {params.fqc} -t {threads} -o {params.output_dir} stdin > {log} 2>&1
        """

rule star:
    input:
        fastq = lambda wildcards: retrieve_fastqs(wildcards.sample)
    output:
        bam = "results/mapped_reads/{sample}.bam",
        sorted_bam = "results/mapped_reads/{sample}.sorted.bam",
        final_log = "results/mapped_reads/{sample}.log.out",
        log = temp("results/mapped_reads/{sample}Log.out"),
        progress = temp("results/mapped_reads/{sample}Log.progress.out"),
        SJ = temp("results/mapped_reads/{sample}SJ.out.tab")
    params:
        index = MAPPER_INDEX,
        star = STAR_EXECUTABLE,
        output_dir = lambda wildcards, output: os.path.join(os.path.dirname(output.bam),  wildcards.sample),
        read1_suffix=READ1_SUFFIX,
        read2_suffix=READ2_SUFFIX
    log: "results/logs/star/{sample}.log"
    threads: NUM_THREADS_PER_SAMPLE 
    shell:
        """
        forward_reads=`ls {input.fastq} | grep {params.read1_suffix}'.{{ cookiecutter.fastq_suffix }}' || printf ' '`
        forward_reads=`echo $forward_reads | paste -d "," -s -`
        reverse_reads=`ls {input.fastq} | grep {params.read2_suffix}'.{{ cookiecutter.fastq_suffix }}' || printf ' '`
        reverse_reads=`echo $reverse_reads | paste -d "," -s -`
        {params.star} --runThreadN {threads} --genomeDir {params.index} --outFileNamePrefix {params.output_dir} --outSAMstrandField intronMotif --outSAMtype BAM SortedByCoordinate Unsorted --readFilesCommand zcat --readFilesIn $forward_reads $reverse_reads  > {log} 2>&1
        
        mv  "results/mapped_reads/{wildcards.sample}Aligned.out.bam" {output.bam}
        mv results/mapped_reads/{wildcards.sample}Aligned.sortedByCoord.out.bam {output.sorted_bam}
        mv results/mapped_reads/{wildcards.sample}Log.final.out {output.final_log}
        """

rule bam_per_species:
    input:
        "results/mapped_reads/{sample}.bam"
    output:
        "results/final_bams/{sample}.{species}.bam"
    threads: NUM_THREADS_PER_SAMPLE
    shell:
        """
        ln -sr {input} {output}
        """

rule index_bam:
    input:
        "results/final_bams/{sample}.{species}.bam"
    output:
        "results/final_bams/{sample}.{species}.sorted.bam.bai"
    params:
        tmp_dir = 'tmp'
    threads: NUM_THREADS_PER_SAMPLE
    shell:
        """
        sambamba sort -t {threads} --tmpdir {params.tmp_dir} {input} {output}
        """

rule feature_counts:
    input:
        bam = "results/final_bams/{sample}.{species}.bam"
    output:
        counts_temp = temp("results/read_counts/{sample}.{species}.counts.tmp"),
        counts_temp_summary = temp("results/read_counts/{sample}.{species}.counts.tmp.summary"),
        counts_out = "results/read_counts/{sample}.{species}.counts",
        counts_summary_out = "results/read_counts/{sample}.{species}.counts.summary"
    threads: NUM_THREADS_PER_SAMPLE
    params:
        strandedness_flag = lambda parameter: strand_test(),
        featurecount = FEATURECOUNTS_EXECUTABLE,
        gtf = lambda wildcards: gtf_dict[wildcards.species]
    log: "results/logs/featureCount/{sample}.{species}.log"
    shell:
        """
        {params.featurecount} -T {threads} -p -a {params.gtf} -o {output.counts_temp} -s {params.strandedness_flag} {input.bam} 2> {log}
        tail -n +3 {output.counts_temp} | cut -f 1,7 > {output.counts_out}
        cp {output.counts_temp_summary} {output.counts_summary_out}
        """

rule create_rrna_intervals:
    input:
        gtf = lambda wildcards: gtf_dict[wildcards.species],
        bam = "results/final_bams/{sample}.{species}.bam"
    output:
        rrna_intervals = "results/alignment_metrics/{species}/{sample}_intervalListBody.txt",
        rrna_header = "data/picard/{species}/{sample}_header.txt",
        sample_rrna = "data/picard/{species}/{sample}.txt"
    threads: NUM_THREADS_PER_SAMPLE
    shell:
        """
        grep rRNA {input.gtf} | cut -s -f 1,4,5,7,9 > {output.rrna_intervals}
        sambamba view -t {threads} -H {input.bam} > {output.rrna_header}
        cat {output.rrna_header} {output.rrna_intervals} > {output.sample_rrna} 
        """

rule run_picard:
    input:
        rrna_intervals = "data/picard/{species}/{sample}.txt",
        bam = "results/final_bams/{sample}.{species}.bam"
    output:
        picard_metrics = "results/alignment_metrics/{species}/{sample}.txt"
    params:
        picard = PICARD_EXECUTABLE,
        ref = lambda wildcards: glob("data/picard/%s/*rff" % wildcards.species ),
        strandedness_flag = lambda parameter: strand_test(picard=True)
    log: "results/logs/picard/{sample}.{species}.log"
    shell:
        """
        java -jar {params.picard} CollectRnaSeqMetrics I={input.bam} O={output.picard_metrics} REF_FLAT={params.ref} STRAND={params.strandedness_flag} RIBOSOMAL_INTERVALS={input.rrna_intervals} 2> {log}
        """


rule bams:
    input:
        indexed_bams = expand("results/final_bams/{sample}.{species}.sorted.bam.bai", sample=SAMPLES, species=SPECIES)

rule multiqc:
    input:
         picard = expand("results/alignment_metrics/{species}/{sample}.txt", sample=SAMPLES, species=SPECIES),
         fc = expand("results/read_counts/{sample}.{species}.counts", sample=SAMPLES, species=SPECIES),
         fastqc = expand("results/fastqc/{sample}/stdin_fastqc.html", sample=SAMPLES)
    output:
        "multiqc_report.html"
    threads: NUM_THREADS_PER_SAMPLE
    params:
        input_dir = "results"
    shell:
        """
        multiqc -d -f -m featureCounts -m star -m fastqc -m salmon -m kallisto -m sargasso -m picard -m bowtie2 {params.input_dir}
        """






